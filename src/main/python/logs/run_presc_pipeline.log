"01-Jan-24 22:26:17" : numexpr.utils : INFO : NumExpr defaulting to 4 threads.
"01-Jan-24 22:26:17" : root : INFO : run_presc_pipeline.py is started...
"01-Jan-24 22:26:17" : root : INFO : main() is started...
"01-Jan-24 22:26:17" : create_objects : INFO : get_spark_object() is started. The 'Test' env is used.
"01-Jan-24 22:26:24" : create_objects : INFO : Spark Object is created
"01-Jan-24 22:26:30" : validations : INFO : Validate the spark object by printing current date : [Row(current_date()=datetime.date(2024, 1, 1))]
"01-Jan-24 22:26:30" : validations : INFO : Spark object is validated and it is ready
"01-Jan-24 22:26:30" : presc_run_data_ingest : INFO : The load_files() is started ...
"01-Jan-24 22:26:31" : presc_run_data_ingest : INFO : The input file file:///home/hadoop/Documents/BigData-Project-End-to-End/src/main/staging/Dim/us_cities_dimension.parquet is loaded to the data frame
"01-Jan-24 22:26:31" : presc_run_data_ingest : INFO : The load_files() is completed ...
"01-Jan-24 22:26:31" : validations : INFO : The DataFrame validation by count df_count() is started for the dataframe df_city ...
"01-Jan-24 22:26:33" : validations : INFO : The dataframe count is 28338.
"01-Jan-24 22:26:33" : validations : INFO : The DataFrame validation by count df_count() is completed.
"01-Jan-24 22:26:33" : validations : INFO : The DataFrame validation by top 10 record df_top10_rec() is started for the dataframe df_city ...
"01-Jan-24 22:26:33" : validations : INFO : The DataFrame top 10 records are:...
"01-Jan-24 22:26:33" : validations : INFO : The DataFrame validation by top 10 record df_top10_rec() is completed.
"01-Jan-24 22:26:33" : presc_run_data_ingest : INFO : The load_files() is started ...
"01-Jan-24 22:26:34" : presc_run_data_ingest : INFO : The input file file:///home/hadoop/Documents/BigData-Project-End-to-End/src/main/staging/Fact/USA_Presc_Medicare_Data_2021.csv is loaded to the data frame
"01-Jan-24 22:26:34" : presc_run_data_ingest : INFO : The load_files() is completed ...
"01-Jan-24 22:26:34" : validations : INFO : The DataFrame validation by count df_count() is started for the dataframe df_fact ...
"01-Jan-24 22:26:34" : validations : INFO : The dataframe count is 105.
"01-Jan-24 22:26:34" : validations : INFO : The DataFrame validation by count df_count() is completed.
"01-Jan-24 22:26:34" : validations : INFO : The DataFrame validation by top 10 record df_top10_rec() is started for the dataframe df_fact ...
"01-Jan-24 22:26:34" : validations : INFO : The DataFrame top 10 records are:...
"01-Jan-24 22:26:34" : validations : INFO : The DataFrame validation by top 10 record df_top10_rec() is completed.
"01-Jan-24 22:26:34" : presc_run_data_preprocessor : INFO : perform_data_clean() is started for df_city dataframe...
"01-Jan-24 22:26:34" : presc_run_data_preprocessor : INFO : perform_data_clean() is started for df_fact dataframe...
"01-Jan-24 22:26:35" : presc_run_data_preprocessor : INFO : perform_data_clean() is completed.
"01-Jan-24 22:26:35" : validations : INFO : The DataFrame validation by top 10 record df_top10_rec() is started for the dataframe df_city_sel ...
"01-Jan-24 22:26:35" : validations : INFO : The DataFrame top 10 records are:...
"01-Jan-24 22:26:35" : validations : INFO : The DataFrame validation by top 10 record df_top10_rec() is completed.
"01-Jan-24 22:26:35" : validations : INFO : The DataFrame validation by top 10 record df_top10_rec() is started for the dataframe df_fact_sel ...
"01-Jan-24 22:26:35" : validations : INFO : The DataFrame top 10 records are:...
"01-Jan-24 22:26:35" : validations : INFO : The DataFrame validation by top 10 record df_top10_rec() is completed.
"01-Jan-24 22:26:35" : root : INFO : run_presc_pipeline.py is completed
"01-Jan-24 22:32:27" : numexpr.utils : INFO : NumExpr defaulting to 4 threads.
"01-Jan-24 22:32:27" : root : INFO : run_presc_pipeline.py is started...
"01-Jan-24 22:32:27" : root : INFO : main() is started...
"01-Jan-24 22:32:27" : create_objects : INFO : get_spark_object() is started. The 'Test' env is used.
"01-Jan-24 22:32:34" : create_objects : INFO : Spark Object is created
"01-Jan-24 22:32:40" : validations : INFO : Validate the spark object by printing current date : [Row(current_date()=datetime.date(2024, 1, 1))]
"01-Jan-24 22:32:40" : validations : INFO : Spark object is validated and it is ready
"01-Jan-24 22:32:40" : presc_run_data_ingest : INFO : The load_files() is started ...
"01-Jan-24 22:32:41" : presc_run_data_ingest : INFO : The input file file:///home/hadoop/Documents/BigData-Project-End-to-End/src/main/staging/Dim/us_cities_dimension.parquet is loaded to the data frame
"01-Jan-24 22:32:41" : presc_run_data_ingest : INFO : The load_files() is completed ...
"01-Jan-24 22:32:41" : validations : INFO : The DataFrame validation by count df_count() is started for the dataframe df_city ...
"01-Jan-24 22:32:42" : validations : INFO : The dataframe count is 28338.
"01-Jan-24 22:32:42" : validations : INFO : The DataFrame validation by count df_count() is completed.
"01-Jan-24 22:32:42" : validations : INFO : The DataFrame validation by top 10 record df_top10_rec() is started for the dataframe df_city ...
"01-Jan-24 22:32:42" : validations : INFO : The DataFrame top 10 records are:...
"01-Jan-24 22:32:43" : validations : INFO : The dataframe count is 28338.
"01-Jan-24 22:32:43" : validations : INFO : The DataFrame validation by top 10 record df_top10_rec() is completed.
"01-Jan-24 22:32:43" : presc_run_data_ingest : INFO : The load_files() is started ...
"01-Jan-24 22:32:44" : presc_run_data_ingest : INFO : The input file file:///home/hadoop/Documents/BigData-Project-End-to-End/src/main/staging/Fact/USA_Presc_Medicare_Data_2021.csv is loaded to the data frame
"01-Jan-24 22:32:44" : presc_run_data_ingest : INFO : The load_files() is completed ...
"01-Jan-24 22:32:44" : validations : INFO : The DataFrame validation by count df_count() is started for the dataframe df_fact ...
"01-Jan-24 22:32:44" : validations : INFO : The dataframe count is 105.
"01-Jan-24 22:32:44" : validations : INFO : The DataFrame validation by count df_count() is completed.
"01-Jan-24 22:32:44" : validations : INFO : The DataFrame validation by top 10 record df_top10_rec() is started for the dataframe df_fact ...
"01-Jan-24 22:32:44" : validations : INFO : The DataFrame top 10 records are:...
"01-Jan-24 22:32:44" : validations : INFO : The dataframe count is 105.
"01-Jan-24 22:32:44" : validations : INFO : The DataFrame validation by top 10 record df_top10_rec() is completed.
"01-Jan-24 22:32:44" : presc_run_data_preprocessor : INFO : perform_data_clean() is started for df_city dataframe...
"01-Jan-24 22:32:44" : presc_run_data_preprocessor : INFO : perform_data_clean() is started for df_fact dataframe...
"01-Jan-24 22:32:45" : presc_run_data_preprocessor : INFO : perform_data_clean() is completed.
"01-Jan-24 22:32:45" : validations : INFO : The DataFrame validation by top 10 record df_top10_rec() is started for the dataframe df_city_sel ...
"01-Jan-24 22:32:45" : validations : INFO : The DataFrame top 10 records are:...
"01-Jan-24 22:32:46" : validations : INFO : The dataframe count is 28338.
"01-Jan-24 22:32:46" : validations : INFO : The DataFrame validation by top 10 record df_top10_rec() is completed.
"01-Jan-24 22:32:46" : validations : INFO : The DataFrame validation by top 10 record df_top10_rec() is started for the dataframe df_fact_sel ...
"01-Jan-24 22:32:46" : validations : INFO : The DataFrame top 10 records are:...
"01-Jan-24 22:32:46" : validations : ERROR : Error in the method df_top10_rec(). 'NoneType' object has no attribute 'count'
"01-Jan-24 22:32:46" : root : ERROR : Error occurred in the main() method. 'NoneType' object has no attribute 'count'
Traceback (most recent call last):
  File "/home/hadoop/Documents/BigData-Project-End-to-End/src/main/python/bin/run_presc_pipeline.py", line 86, in main
    df_top10_rec(df_fact_sel, 'df_fact_sel')
  File "/home/hadoop/Documents/BigData-Project-End-to-End/src/main/python/bin/validations.py", line 43, in df_top10_rec
    dfcount = df.count()
AttributeError: 'NoneType' object has no attribute 'count'
"01-Jan-24 22:33:57" : numexpr.utils : INFO : NumExpr defaulting to 4 threads.
"01-Jan-24 22:33:57" : root : INFO : run_presc_pipeline.py is started...
"01-Jan-24 22:33:57" : root : INFO : main() is started...
"01-Jan-24 22:33:57" : create_objects : INFO : get_spark_object() is started. The 'Test' env is used.
"01-Jan-24 22:34:03" : create_objects : INFO : Spark Object is created
"01-Jan-24 22:34:09" : validations : INFO : Validate the spark object by printing current date : [Row(current_date()=datetime.date(2024, 1, 1))]
"01-Jan-24 22:34:09" : validations : INFO : Spark object is validated and it is ready
"01-Jan-24 22:34:09" : presc_run_data_ingest : INFO : The load_files() is started ...
"01-Jan-24 22:34:10" : presc_run_data_ingest : INFO : The input file file:///home/hadoop/Documents/BigData-Project-End-to-End/src/main/staging/Dim/us_cities_dimension.parquet is loaded to the data frame
"01-Jan-24 22:34:10" : presc_run_data_ingest : INFO : The load_files() is completed ...
"01-Jan-24 22:34:10" : validations : INFO : The DataFrame validation by count df_count() is started for the dataframe df_city ...
"01-Jan-24 22:34:12" : validations : INFO : The dataframe count is 28338.
"01-Jan-24 22:34:12" : validations : INFO : The DataFrame validation by count df_count() is completed.
"01-Jan-24 22:34:12" : presc_run_data_ingest : INFO : The load_files() is started ...
"01-Jan-24 22:34:13" : presc_run_data_ingest : INFO : The input file file:///home/hadoop/Documents/BigData-Project-End-to-End/src/main/staging/Fact/USA_Presc_Medicare_Data_2021.csv is loaded to the data frame
"01-Jan-24 22:34:13" : presc_run_data_ingest : INFO : The load_files() is completed ...
"01-Jan-24 22:34:13" : validations : INFO : The DataFrame validation by count df_count() is started for the dataframe df_fact ...
"01-Jan-24 22:34:13" : validations : INFO : The dataframe count is 105.
"01-Jan-24 22:34:13" : validations : INFO : The DataFrame validation by count df_count() is completed.
"01-Jan-24 22:34:13" : presc_run_data_preprocessor : INFO : perform_data_clean() is started for df_city dataframe...
"01-Jan-24 22:34:13" : presc_run_data_preprocessor : INFO : perform_data_clean() is started for df_fact dataframe...
"01-Jan-24 22:34:14" : presc_run_data_preprocessor : INFO : perform_data_clean() is completed.
"01-Jan-24 22:34:14" : root : INFO : run_presc_pipeline.py is completed
